{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e183096c",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d15b3",
   "metadata": {},
   "source": [
    "https://sebastianraschka.com/Articles/2014_python_lda.html"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3114cead",
   "metadata": {},
   "source": [
    "- Used as a dimensionality reduction technique\n",
    "\n",
    "- Used in the preprocessig step for pattern classification and ML ALgorithms\n",
    "\n",
    "- Has the goal to project a dataset onto a lower-dimensional space"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05598604",
   "metadata": {},
   "source": [
    "It is similar to PCA but there is a major difference  which is \n",
    "\n",
    "LDA differs because in the addition to finding the component axises with LDA we are interseted in the axes that maximize the separation between multiple classes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4b05fc0",
   "metadata": {},
   "source": [
    "Breaking it down Funrther\n",
    "\n",
    "The goal of LDA is to project a feature space (a dataset n-dimensional samples) onto a small subspace k(where k<=n-1) while maintaning the class-discriminatory information.\n",
    "\n",
    "Both PCA nad LDA are linear transformation techniques used for dimensional reduction. PCA is described as unsupervised but LDA is supervised because of the relation to the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ff8f7",
   "metadata": {},
   "source": [
    "## Differences of PCA and LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a6834",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:1192/1*vXQ5sgMF0XmiY4Jc6gJVwA.png\" jsaction=\"VQAsE\" class=\"r48jcc pT0Scc iPVvYb\" style=\"max-width: 1192px; height: 235px; margin: 11px 0px; width: 461px;\" alt=\"Illustrative Example of Principal Component Analysis(PCA) vs Linear  Discriminant Analysis(LDA): Is PCA good guy or bad guy ? | by gopi sumanth  | Analytics Vidhya | Medium\" jsname=\"kn3ccd\" aria-hidden=\"false\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7178401a",
   "metadata": {},
   "source": [
    "1. Compute the d-dimensional mean vectors for the different classes from the dataset.\n",
    "2. Compute the scatter matrices (in-between-class and within-class scatter matrix).\n",
    "3. Compute the eigenvectors (e1,e2,...,ed) and corresponding eigenvalues (λ1, λ2,..., λd) for the scatter matrices.\n",
    "4. Sort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with the largest eigenvalues to form a d×k dimensional matrix W (where every column represents an eigenvector).\n",
    "- Use this d × k eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the matrix multiplication: Y = X × W (where X is a n × d-dimensional matrix representing the n samples, and y are the transformed n×k - dimensional samples in the new subspace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eadf18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
