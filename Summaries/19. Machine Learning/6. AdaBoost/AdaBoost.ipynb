{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3f3ec0f1",
   "metadata": {},
   "source": [
    "Ensembl Technique :- \n",
    "    Bagging >> Parallel Approche\n",
    "    Boosting >> Sequencial Approche"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83c3046e",
   "metadata": {},
   "source": [
    "Algorithms who follow bosting\n",
    "    - 1. AdaBoost\n",
    "    - 2. XGBoost\n",
    "    - 3. GradientBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f0f38",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b37431",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b77f04a7",
   "metadata": {},
   "source": [
    "In Adaboost, Boosting is a sequential ensemble method that iteratively adjusts the weight of observation as per the last classification. If an observation is incorrectly classified, it increases the weight of that observation. The term ‘Boosting’ in a layman language, refers to algorithms that convert a weak learner to a stronger one. It decreases the bias error and builds strong predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e18057",
   "metadata": {},
   "source": [
    "## Working"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fcb00b6d",
   "metadata": {},
   "source": [
    "- 1. It is a supervised Machine Learning Algorithm, which is usefull for classification as well as regression, it follows Sequential Approche, in Adaboost a weak learner will be converted into strong learner\n",
    "\n",
    "- 2. In AdaBoost algorithm, all the random data will be shared to base learner 1 (WL1), and Initial weights will get assigned to each data points (1/n), where n = number of row records or samples. for each data points weights will be simillar at initial stage.\n",
    "\n",
    "- 3. Weak learner (WL1) will train on the shared data by Original dataset. and it will build a STUMPS (Bessicaly a tree with one branch node and Two leaf nodes). WL1 will return some outputs. some outputs will be correctly classified and some ouputes will be incorrectly classified.\n",
    "\n",
    "- 4. Then for WL1 model performance will be calculated, Then New Sample weights will be calculated for each data points, In Addition New Sample weights will get assigned to the each data points \n",
    "     in such a way that, for Correctly classified data points the weights will be decreased and for missclassified data points Weights will get increase.\n",
    "     \n",
    "- 5. after that the new samples will get normalized, and this normalized weights will be the updated weights for the WL2, then Buckets will we formed (Range 0 to 1).\n",
    "\n",
    "- 6. Random values will be generated for each records or row the random values ranges between 0  to  1. on the basis of randome values records will be picked up from the WL1 for Creating a WL2.\n",
    "     \n",
    "     Note :- Some of the recored will be repeated and some recored will be unique, but all the recored will be taken from WL1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3749a4b",
   "metadata": {},
   "source": [
    "### Steps :- \n",
    "1. https://www.kdnuggets.com/2020/12/implementing-adaboost-algorithm-from-scratch.html\n",
    "2. https://towardsdatascience.com/multiple-model-creation-using-adaboost-technique-2119b7aacf25#:~:text=Step%206%3A%20Creation%20of%20new%20dataset%20as%20per%20bucket&text=To%20achieve%20the%20same%20we%20create%20a%20bucket%20of%20ranges,to%20the%20random%20value%20generated.&text=Now%20we%20run%20a%20loop,classify%20it%20w.r.t%20the%20bucket."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0702a6f",
   "metadata": {},
   "source": [
    "1. Initialize the dataset and assign the initial weights (1/n)\n",
    "2. offer this data to the model 1 (WL1).\n",
    "3. make predictions and detect the missclassification datapoints from the WL1.\n",
    "4. Calculate the Total Error\n",
    "5. Calculate the Model Performance\n",
    "6. update the new weights, For Missclassification Weights will be increased and for Classified data  points Weights will be decreased.\n",
    "7. Normalize the weights (Sum of all the observations should be 1).\n",
    "8. create a Bucket ranges from 0 to 1 \n",
    "9. assign Random weights to each observations\n",
    "10. Pick the recoreds form the WL1 for WL2 model based on the rang of the bucket\n",
    "11. This procedure is continued until and unless the errors are minimized, and the dataset is predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c9b3c",
   "metadata": {},
   "source": [
    "## Formulas :- "
   ]
  },
  {
   "cell_type": "raw",
   "id": "de942f8b",
   "metadata": {},
   "source": [
    "- 1. Sample weight = 1/N\n",
    "    Where N = Number of records\n",
    "    \n",
    "- 2. Total Errors :-  \n",
    "    Total Error (TE) = sum weights of misclassified data points / Total Number of data points\n",
    "\n",
    "- 3. Model Performance :- \n",
    "    Model Performance = 1/2 * loge((1-TE)/TE)\n",
    "    \n",
    "- 4. New Sample Weights of Correctly classified and Incorrectly classified\n",
    "\n",
    "    \n",
    "    New_Sample_Weights (Misclassified Data points) = old weight * E^(Model Performance)\n",
    "    \n",
    "    New_Sample_Weights (Correctly Classified Datapoints) = old weight * E^(-Model Performance)\n",
    "    \n",
    "5.  Normalized weight :-\n",
    "    Normalized weight = New weight / sum of new weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d2c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5be3fd0f",
   "metadata": {},
   "source": [
    "### hyperparameter Tuning for Adaboost"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2bcb2cf",
   "metadata": {},
   "source": [
    "1. estimator = Classifier_name\n",
    "2. n_estimators=50,\n",
    "3. learning_rate=[0.001 to 1.0],\n",
    "4. algorithm= ['SAMME.R', 'SAMME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba4961",
   "metadata": {},
   "source": [
    "### Advantages"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42bc46b6",
   "metadata": {},
   "source": [
    "- 1. Adaboost is less prone to overfitting as the input parameters are not jointly optimized\n",
    "- 2. The accuracy of weak classifiers can be improved by using Adaboost. Nowadays, Adaboost is being used to classify text and images rather than binary classification problems.\n",
    "- 3. It has been extended to learning problems beyond binary classification (i.e.) it can be used  with text or numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35899fb",
   "metadata": {},
   "source": [
    "### Drawbacks"
   ]
  },
  {
   "cell_type": "raw",
   "id": "953e878d",
   "metadata": {},
   "source": [
    "- 1. AdaBoost can be sensitive to noisy data and outliers.\n",
    "- 2. Weak classifiers being too weak can lead to low margins and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f5db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
