{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98315eea",
   "metadata": {},
   "source": [
    "### Linear Regression "
   ]
  },
  {
   "cell_type": "raw",
   "id": "296c69ab",
   "metadata": {},
   "source": [
    "- It is a supervised Machine Learning algoritham, it is a predictive model which is use to finding the reletionship between One More Independent and Dependent variables\n",
    "\n",
    "- The Purpose of the Linear regression is to Find the Best fit line and reduce the MSE (Mean squared Error).\n",
    "\n",
    "- We will be having labled data in our dataset \n",
    "     - We will have a Contineous Target column. \n",
    "         - target >> dependent >> output variable -- Y-axis.\n",
    "     \n",
    "    - we will have a Contineous or Discrete Columns.\n",
    "        - Predictores >> Independent >> Input variables -- X-axis.  \n",
    "\n",
    "- In Linear regression we use Mean squared error (MSE) as cost function or Loss Function\n",
    "\n",
    "- There are Three Types of Linear Regression's\n",
    "    - 1. Simple Linear Regression (It finds the regression line on OLS method least ordinary squared mehtod)\n",
    "            - Equation :- y = mx + c\n",
    "    - 2. Multiple Linear Regression\n",
    "            - Equation :- y = m1x1 + m2x2 + m3x3 + m4x4 +.... mnxn + c\n",
    "    - 3. Polynomial Linear Regression\n",
    "    \n",
    "m - slope\n",
    "c - intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d9f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1470a6cf",
   "metadata": {},
   "source": [
    "### Best Fit Line"
   ]
  },
  {
   "cell_type": "raw",
   "id": "865635d3",
   "metadata": {},
   "source": [
    "- Best fit line is also known as regression Line which is passes through the maximum data point's.\n",
    "- Best fit line is the line where the MSE or SSE is is low (Mean Squared Error or Sum Of Squared Error)\n",
    "- Gradient Decent Algorithm will help us to get the Best fit line\n",
    "- Gradient Descent Algorithm will find the Best fit line among the possibilities or n-number of lines which passes through the data points on x-y plane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d59deb",
   "metadata": {},
   "source": [
    "### Error Rate or Residuals"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c4b99c4",
   "metadata": {},
   "source": [
    "- Error Rate is also Known as Residuals.\n",
    "- it is the distance between Yactual and Ypredict Point's.\n",
    "- These residuals need's to be minimise to increase the Accuracy Of our Model.\n",
    "- Gradient Descent Algorithm will help us to reduce the Error Rate.\n",
    "\n",
    "- Possitive points :- if the data points are above the line\n",
    "- Negative points :- if the data points are bellow the line.\n",
    "- Zero :- If the Data Points are on the regression Line are 0th pposition line.\n",
    "\n",
    "Equation:-\n",
    "    - sum(Ya - Yp)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f467f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63932783",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "- There are some assumption to follow where before bulding our model\n",
    "- 1. Linearity\n",
    "- 2. Independence\n",
    "- 3. No or littil Multicolinearity \n",
    "- 4. Normality\n",
    "- 5. homoscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b68f4f",
   "metadata": {},
   "source": [
    "####  1. Linearity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6e91ba4",
   "metadata": {},
   "source": [
    "- It states that the Releationship between Dependent and Independent variables should be Linear\n",
    "- To check the linearity we check for the Cofficient of Corelation or Pearson Cofficient Correlation also denoted by R or rxy\n",
    "\n",
    "- Pearson Cofficient Correlation is directly proportional to Covariance and Inversaly Proportional to Standard Deviation\n",
    "\n",
    "- Formula :-\n",
    "    rxy = sum(Xi - mean(X)) * sum(Yi - mean(Y)) / √sum(Xi - mean(X)) * sum(Yi - mean(Y))\n",
    "    \n",
    "- according to standard  R-values >>> -1 to 1\n",
    "\n",
    "- R-value >> 0.7 to 1  (Good Predictors)\n",
    "- R-value >> -0.7 to -1 (Good Predictors)\n",
    "\n",
    "- R-value >> -0.3 to 0.3 (Bad Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0595f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01c62939",
   "metadata": {},
   "source": [
    "#### 2. Independence"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60739120",
   "metadata": {},
   "source": [
    "- It States that there should not be the relationship between Independent variables, Although there should be the relation between Independent and dependent variables.\n",
    "    - Xn ≠ Xn, but Xn = Yn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4cfe79",
   "metadata": {},
   "source": [
    "#### 3. No or Little Multicolinearity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de2fd69b",
   "metadata": {},
   "source": [
    "- It states the Independent Columns should not be Highly Correlated to each other.\n",
    "\n",
    "- To Check the multi-colinearity between Independent Features we can check the VIF (Variance Inflation Factors) or we can get the heatmap from seaborn library to check the coifficient of corelation.\n",
    "\n",
    "- VIF is use to detect the presence of MultiColinearity in our dataset.\n",
    "\n",
    "- Correlation coefficients whose magnitude are between 0.7 and 0.9 indicate variables which can be considered highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd791a",
   "metadata": {},
   "source": [
    "#### 4. Normality "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fde92c9",
   "metadata": {},
   "source": [
    "- it has a bellshape curve and also known as normal distribution curve.\n",
    "- Normality is the Normal Distribution of the Error or residuals\n",
    "- the residuals should be near or around the mean values (mean = 0)\n",
    "- there are three std 1st, 2nd, 3rd std \n",
    "- the residuals should be within 1st,2nd and 3rd deviation.\n",
    "- If the Errors or residuals are away from the 3std then it is considerd as Outliears\n",
    "\n",
    "- Outliars\n",
    "    - 1. Normal outliars >> near the 3rd std\n",
    "    - 2. Extreme outliars >> away from the all std's\n",
    "    \n",
    "- Standard Deviation (std)\n",
    "- Standard Deviation will help us to identify the spread of the Data Points.\n",
    "\n",
    "- Low Standard Deviation >> LSD in which most of the data points wiill be closed to the mean value (This is the Expected Case)\n",
    "\n",
    "- High STD >> Values / Data Points are far away from the Mean Value (We always try to reduce the impact of data points or errors or Residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8177758",
   "metadata": {},
   "source": [
    "#### 5. Homoscedasticity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38483cae",
   "metadata": {},
   "source": [
    "- It states that The spread of our data should be in a constant rate or uniformly distributed on the plane. \n",
    "\n",
    "- If we do get the variations in the residuals or the data points from the regression line. Then that will be Heteroscedasticity.\n",
    "\n",
    "- Homoscedasticity in a model means that the error is constant along the values of the dependent variable. The best way for checking homoscedasticity is to make a scatterplot with the residuals against the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8380333",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042a89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = sum(Ya - Yp)^2/n-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5217c5c",
   "metadata": {},
   "source": [
    "### Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "809e57a1",
   "metadata": {},
   "source": [
    "- Gradient Descent Algorithm is use to reduce the cost function also known as MSE or loss function.\n",
    "- It works on partial derivative.\n",
    "\n",
    "- Whenever we will get the partial derivates with respect to M, In that case we can get the value of M and C along with that, we can get the value of C for doing the partial derivates with respect to C.\n",
    "    - 1. Mnew = Mold - Learning_rate * derivative(MSE) / derivative(M)\n",
    "    - 2. Cnew = Cold - Learning_rate * derivative(MSE) / derivative(C)\n",
    "    \n",
    "        derivative(MSE) / derivative(M) ==> Slope\n",
    "        derivative(MSE) / derivative(C) ==> Slope\n",
    "    \n",
    "- by Gradient Descent Algorithm we can get the best M and C value at globle minima.\n",
    "- to achive the globle minima in GDA, the Yactual Point converges to Ypredict point with learning rate = 0.001 \n",
    "\n",
    "- we can get the globle minima where the MSE is lowest. \n",
    "\n",
    "- learning rate should be 0.001 to converge the globle minima however if we excede the limit (L=1 or 2) then we might overshout our global minima. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d466164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6ee954c",
   "metadata": {},
   "source": [
    "### Cost Function / Loss Function / MSE (Mean squared value)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "416da6a1",
   "metadata": {},
   "source": [
    "- MSE has a bowl shape curve or convex curve, it is a part of Gradient decendent algorithm\n",
    "- The purpose of the MSE is to find the lowest MSE value. \n",
    "- it follows the learning rate to get lowest MSE value or Globle minima\n",
    "        - ŋ = 0.001\n",
    "- Equation:- \n",
    "        - sum(Ya - Yp)^2 / N\n",
    "        N - Number of sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949a321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a32eb74",
   "metadata": {},
   "source": [
    "### Feature scalling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c419963c",
   "metadata": {},
   "source": [
    "- we use Feature Scalling in a Distance base Algorithm\n",
    "- Feature Scaling is a technique to standardize the independent features present in the data in a fixed range.\n",
    "\n",
    "- Withoud feature scalling our machine learning algorithm will not perform well when the input numerical attributes have very different scales.\n",
    "\n",
    "- A significant issue is that the range of the variables may differ a lot. Using the original scale may put more weights on the variables with a large range. In order to deal with this problem, we need to apply the technique of features rescaling to independent variables or features.\n",
    "\n",
    "- after feature scalling our mean = 0 and std = 1\n",
    "\n",
    "- There are two Techniques to to perfrom featrue scalling\n",
    "    1. standardization >> Z score Normalization\n",
    "    2. normalization >> min-max Normalization\n",
    "\n",
    "1. Xstd          =     X - mean(X) / std(X)\n",
    "\n",
    "2. Normalization =     X - min(X) / max(X) - min(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7140fe05",
   "metadata": {},
   "source": [
    "### when to use Feature Scalling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0461e976",
   "metadata": {},
   "source": [
    "\n",
    "Some machine learning models are fundamentally based on distance matrix, also known as the distance-based classifier, for example, K-Nearest-Neighbours, SVM, and Neural Network. Feature scaling is extremely essential to those models, especially when the range of the features is very different. Otherwise, features with a large range will have a large influence in computing the distance.\n",
    "\n",
    "\n",
    "- Normalization :- \n",
    "Max-Min Normalisation typically allows us to transform the data with varying scales so that no specific dimension will dominate the statistics, and it does not require making a very strong assumption about the distribution of the data, such as k-nearest neighbours and artificial neural networks. However, Normalisation does not treat outliners very well.\n",
    "\n",
    "\n",
    "- Standardization:- \n",
    "standardisation allows users to better handle the outliers and facilitate convergence for some computational algorithms like gradient descent. Therefore, we usually prefer standardisation over Min-Max Normalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae2b0d",
   "metadata": {},
   "source": [
    "### When Not to use feature scalling\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e723c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a1dc858",
   "metadata": {},
   "source": [
    "### Evaluation of a model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cd5ec72",
   "metadata": {},
   "source": [
    "- 1. SSE ~ RSS (Sum of squared Error / Residuals Sum Of Square)\n",
    "    - SSE = sum(Ya - Yp)^2\n",
    "    \n",
    "- 2. SSR (Sum of squared Error Due to Regression line)\n",
    "    - SSR = sum(Yp - ȳ)^2\n",
    "      ȳ - mean(Y)\n",
    "    \n",
    "- 3. SST (Total Sum of squared Error)\n",
    "    - SST = sum(Ya - ȳ)^2\n",
    "      ȳ - mean(Y)\n",
    "      \n",
    " \n",
    "- inbuilt evaluation\n",
    "    - MSE (mean squared Error)\n",
    "    - MEA (mean Absolute Error)\n",
    "    - RMSE (Root of mean squared Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cf976",
   "metadata": {},
   "source": [
    "####  r2_score"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e43fda0e",
   "metadata": {},
   "source": [
    "- r2_score is also known as Cofficient of Determination.\n",
    "- r2_score is a scale invarient\n",
    "- the values are lie between (0-1)\n",
    "- it is a inbuilt library of sklearn\n",
    "\n",
    "r2_score = 0 (worst score)\n",
    "r2_score = 1 (Good score)\n",
    "\n",
    "our accuracy should be above 70%\n",
    "\n",
    "- Equations:-\n",
    "     r2_score = 1- SSE / SST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a9a3f",
   "metadata": {},
   "source": [
    "####  Adjusted r2_score"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bc4c472",
   "metadata": {},
   "source": [
    "Equation:- \n",
    "     Adjusted R2    = R2_score - ( k - 1 ) * ( 1 - (R2_score)^2 ) / ( n - k) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18882e",
   "metadata": {},
   "source": [
    "### Difference between R2_score and Adjusted R2_score"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62f4050b",
   "metadata": {},
   "source": [
    "R2_Score :-\n",
    "    - it is inbuilt in sklearn.metrecs library \n",
    "    - whenever we add coreleted data into the dataset our r2_score of our model will be increase.\n",
    "    - but when we add non-correlated data into the dataset our r2_score of our model will remain same as before.\n",
    " \n",
    " \n",
    " Adjusted R2_Score :- \n",
    "     - we have to manually calculate it \n",
    "    - whenever we add co-releted data into the dataset our Adjusted r2_score of our model will be increase.\n",
    "    but when we add non-correlated data into the dataset our Adjusted r2_score of our model will get Decrease.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c09b37",
   "metadata": {},
   "source": [
    "###  Bias and Variance Trade-Of"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b9259e2",
   "metadata": {},
   "source": [
    "- if the machine learning model is not accurate, it can make predictions errors, and these prediction errors are usually known as Bias and Variance. \n",
    "\n",
    "- In machine learning, these errors will always be present as there is always a slight difference between the model predictions and actual predictions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "57080aa0",
   "metadata": {},
   "source": [
    "Bias :-\n",
    "    - it can be defined as an inability of machine learning algorithms such as Linear Regression to capture the true relationship between the data points."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfb5af55",
   "metadata": {},
   "source": [
    "\n",
    "- Bias - it is An Error of the Training data points.\n",
    "- Variance - It is an Error of the Test data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c06550f",
   "metadata": {},
   "source": [
    "###### 1.  ***Underfitting***"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ce3fabd",
   "metadata": {},
   "source": [
    "- if the regression line is passing through some training data points and some test data pioints and there they have maximum error rate in Training set. and low error rate in Test set.\n",
    "- Underfiting\n",
    "    - (max Error in Training data points)High Bias & \n",
    "    - (Min error rate in Test data points) Low Variance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84539f3",
   "metadata": {},
   "source": [
    "###### 2. ***Overfitting***"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c48462f",
   "metadata": {},
   "source": [
    "- if the regression line is passing through all training data points with no error and have Maximum Error rate in test data pioints.\n",
    "- Overfitting\n",
    "    - (max Error in Training data points) Low Bias & \n",
    "    - (Min error rate in Test data points) High Variance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45845f31",
   "metadata": {},
   "source": [
    "###### 3. Over model should be with Low bias and Low variance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d89a379",
   "metadata": {},
   "source": [
    "- low Bias and Low Variance can be achive when there is min. error rate in training set data points and min. error rate in test dataset points.\n",
    "- To get min. error rates in both test and training data points we can use L1 & L2 Regularisation mehtod also known as Lasso Regularisation and Ridge Regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c39b3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7085213c",
   "metadata": {},
   "source": [
    "### Lasso Regression (L1 - Regularisation)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8060da5",
   "metadata": {},
   "source": [
    "- When we have 60 to 100 Features in our dataset in that case we can use L1- Regularisation also known as Lasso regression.\n",
    "\n",
    "- It is also use as a feature selection technique\n",
    "    - λ = 0.01 to ∞\n",
    "\n",
    "- when λ = 0 then it act like a linear regression\n",
    "\n",
    "- Formula :-\n",
    "    - [ (Ya - Yp)^2 + λ * | θ | ]\n",
    "θ = slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532f00d",
   "metadata": {},
   "source": [
    "### Ridge Regression (L2 - Regularisation)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9932779d",
   "metadata": {},
   "source": [
    "- we can use Ridge Regression (L2 Regularisation) when we are facing Overfitting condition\n",
    "\n",
    "- Formula :-\n",
    "    - [ (Ya - Yp)^2 + λ * (θ)^2 ]\n",
    "θ = slope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
