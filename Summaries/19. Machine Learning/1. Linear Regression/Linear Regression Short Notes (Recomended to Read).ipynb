{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58147ead",
   "metadata": {},
   "source": [
    "### - Linear Regression :- "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac1428f9",
   "metadata": {},
   "source": [
    "- It is a Algorithm based on Gradient Descent Optimisation algorithm. \n",
    "- Linear regression is a Supervised machine Learning Algorithm, it is use to resolve the regression problem, it is a predictive model which is use to finding the relationship between dependent (Y) and independent varables (X).\n",
    "- the dependent variable for regression problem will be Contineoues in nature\n",
    "- Equestion \n",
    "     - Simple linear regression\n",
    "         - Y = Mx + C\n",
    "    - MultiLinear Regression\n",
    "        - Y = M1X1 + M2X2 + M3X3 + ... MnXn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba00e1",
   "metadata": {},
   "source": [
    "#### - Best Fit Line"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70205f76",
   "metadata": {},
   "source": [
    " - it is a line which passes through maximum number of data poits where the cost function or sum of squared error is minimul.\n",
    " - in order to achive best fit line, our model follows Ordinary Least Squared method (OLS) and Gradient Descent Optimization algorithm in backend.\n",
    " \n",
    "- In Multiple liear regression there will be number of regression lines posibilities for each variables. \n",
    "\n",
    "- Equestion - \n",
    "    - Sum(Ya - Yp)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab2706",
   "metadata": {},
   "source": [
    "### - Error Rates or Residuals"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3194f960",
   "metadata": {},
   "source": [
    "- it is the distance between Yactual and Ypredicted points.\n",
    "- These Error Rates should be reduce in order to achive good accuracy score of the model.\n",
    "- Equestion \n",
    "        - Sum(Yactual - Ypredict)\n",
    "        \n",
    "- The data points which are above the regression line known as positive data points\n",
    "- the data points which are bellow the regression line known as negative points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8853352c",
   "metadata": {},
   "source": [
    "### - Assumptions made in Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b42b74",
   "metadata": {},
   "source": [
    "- 1. Linearity \n",
    "- 2. Independence \n",
    "- 3. No Multicolinearity\n",
    "- 4. Normality of residuals\n",
    "    - Example :- https://www.statology.org/example-of-normal-distribution/\n",
    "- 5. Homoschedasticity\n",
    "    - Example :- https://www.investopedia.com/terms/h/homoskedastic.asp"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc6d947f",
   "metadata": {},
   "source": [
    "- 1. Linearity :-\n",
    "\n",
    "    Y = M1x1 +M2x2 + M3X3...MnXn\n",
    "    The Regression Coefficient in the above equation talks about the change in the value of dependent variable corresponding to the unit change in the independent variable. So, for e.g. if X1 increases or decreases by one unit, then Y will increase or decrease by M1 units. An important assumption followed by an ideal linear regression is that any increase or decrease in one independent variable will not have any corresponding changes in other independent variables.\n",
    "\n",
    "    In linearity we make sure that the relationship between Independent (X) and Dependent (Y) Variables should be Linear \n",
    "- To check linearity 1) Pearson cofficient of correlation\n",
    "        - rxy = covariance/std\n",
    "        - rxy = Sum(Xi-mean(Xi))*(Yi - mean(Yi)) / √Sum(Xi-mean(Xi))^2 * (Yi - mean(Yi))^2\n",
    "        \n",
    "- Ex. as 1) X↑ Y↑ simultaniously (Y increasing in Positive dirs)\n",
    "         2) X↑ Y↓ simultaniously (Y increasing in Negative dirs)\n",
    "         \n",
    "- R-values = -1 to +1\n",
    "    - positive direction 0.7 to 1 (Good predictiors)\n",
    "    - negative direction -0.7 to -1 (Good predictiors)    \n",
    "    \n",
    "    - positive direction -0.3 to 0.3 (Bad predictiors)\n",
    "    \n",
    "- Crecket score, sales, Exprience vs Salary "
   ]
  },
  {
   "cell_type": "raw",
   "id": "09ac76a4",
   "metadata": {},
   "source": [
    "- 2. Independence\n",
    "\n",
    "    - It states that there should not be the linear relationship between Independent or Input variables (X & X), but there should be the linear relationship between input and output variables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d89d6c24",
   "metadata": {},
   "source": [
    "- 3. No or Little Multicolinearity\n",
    "\n",
    "    - It state the Input variables (X) should not be highly correlated to each other.\n",
    "    \n",
    "    - if there is any Corelationship then, it would be very defficult to find which input variable has contribution in predicting output variable.\n",
    "    \n",
    "    - however, if there is multicolinearity, then it is posible that those two variables has at some amount of simillar data.\n",
    "    \n",
    "    - Check for the Variance Inflation factor (VIF)\n",
    "    - Vif  = 1 (No Multicolinearity)\n",
    "    - Vif = 1 to 5 (Best)\n",
    "    - Vif = 5 to 10 (Moderate)\n",
    "    - Vif > 10 (presence of Multicolineariy)\n",
    "\n",
    "    \n",
    "- Example :-\n",
    "    - X1 ↑  X2 ↑\n",
    "    - X1 ↑  X2 ↓"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc920208",
   "metadata": {},
   "source": [
    "- Normality Of Residuals :-\n",
    "\n",
    "\n",
    "    - It state that, The Residuals or data points should be normally distributed\n",
    "    - It is the Normal Distribution of the Residuals. \n",
    "    - the residuals should be near to mean or should be lies within (1, 2, 3, 0 -1, -2, -3)STD's\n",
    "    - if the residuals are away from the STD's then it is considerd as Outliars\n",
    "        - Outliars :- 1. Normal Outliars (near the 3rd STD)\n",
    "                      2. Extreme Outliars (Far away from 3rd STD)\n",
    "\n",
    "- Example :- It’s well-documented that the birthweight of newborn babies is normally distributed with a mean of about 7.5 pounds. birthweight of babis is range(6.7 to 8.5)\n",
    "                      \n",
    "\n",
    "- Standard Deviation will help us to identify the spread of the Data Points.\n",
    "\n",
    "    - Low Standard Deviation >> LSD in which most of the data points wiill be closed to the mean value (This is the Expected Case)\n",
    "\n",
    "    - High STD >> Values / Data Points are far away from the Mean Value (We always try to reduce the impact of data points or errors or Residuals)\n",
    "\n",
    "to check for Normal distribution of the resuduals\n",
    "    - 1. Visualization\n",
    "        - 1. histplot\n",
    "        - 2. kdeplot\n",
    "    - 2. Skewness test\n",
    "        - 1. skew > 1.0 (Right skewed) -- log transformation will help us to make data normally distributed\n",
    "        - 1. skew < -1.0 (Left skewed)\n",
    "    - 3. QQ-Plot\n",
    "        - all the points should be near or on the 45 degree stright line\n",
    "    - 4. Hypothesis Test (Check For p_values)\n",
    "        - 1. Shapiro test \n",
    "        - 2. kstest\n",
    "        - 3. normal test\n",
    "        - P_value >= 0.05 (Null Hypothesis All residuals Normally Distributed)\n",
    "        - P_value < 0.05 (Alternate Hypothesis All residuals are Not Normally Distributed)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2877e1b",
   "metadata": {},
   "source": [
    "###### P_Value :- it is define as the probability for the null hypothesis to be true\n",
    "###### Null-Hypothesis :- It Treats everything same or equal"
   ]
  },
  {
   "cell_type": "raw",
   "id": "058ad73c",
   "metadata": {},
   "source": [
    "- Homoscedasticity :-\n",
    "    - the spread of our data should be in a constant Variance, if the it is not in constant variance then it is considerd as Heteroscedasticity\n",
    "    \n",
    "- Example :-\n",
    "    - let's consider a trafic police gathard a data, and according to data people with age group between 18-25, they violates the rules and regulation of traffic (SOme people's) however, the age group (25 - 40) some people violates the rules and some people not. So This is the case of Heteroscedasticity.\n",
    "\n",
    "- people who follow the rules are know as homoscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600fb2a",
   "metadata": {},
   "source": [
    "### - Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ab31c96",
   "metadata": {},
   "source": [
    "- MSE is also known as Cost Function or Loss Function or risk function,\n",
    "\n",
    "- The Mean Squared Error measures how close a regression line is to a set of data points. It is a risk function corresponding to the expected value of the squared error loss.\n",
    "\n",
    "- to reduce the error rates or residuals this MSE function is implimented in a Gradient Descent Algorithm, this function squares the Sum of distance between Observed values of Y and Predicted value of Y and devide it by number of samples (N).\n",
    "\n",
    "-  the MSE cost function for a Linear Regression model happens to be a convex function. the cost function has the shape of a bowl, but it can be an elongated bowl if the features have very different scales. (If not applied Feature scalling)\n",
    "\n",
    "- which means that if you pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum.\n",
    "\n",
    "- In statistics, the mean squared error (MSE) is a risk function that measures the square of errors. When performing regression, use MSE if you believe your target is normally distributed and you want large errors to be penalized more than small ones.\n",
    "\n",
    "-  It is also a continuous function with a slope that never changes abruptly.\n",
    "\n",
    "- Equestion :- \n",
    "    - Sum(Yactual - Ypredicted)^2 / N\n",
    "        - N = Number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca496c0",
   "metadata": {},
   "source": [
    "#### - Gradient Descent Algorithm\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f43a4059",
   "metadata": {},
   "source": [
    "- 1. It is used to reduce the cost function or mean squared error\n",
    "- 2. It will help us to get the best M and C value\n",
    "- 3. Gradient Descent algorithm do work on Partial Derivates.\n",
    "- 4. Whenever we will get the partial derivates with respect to M, In that case we can get the value\n",
    "of M and C along with that, we can get the value of C for doing the partial derivates with respect to  C.\n",
    "- 5. To achive the minimun mean square error of get the best M & C value on the global minima we follow the baby step to converge the Actual points to predict points.\n",
    "- 6. the baby steps is known as learning rate which should be equal to 0.001,\n",
    "- 7. by taking such a small steps there is high chaces to get the mean square error (MSE) but it  consumes more time.\n",
    "- 8. but if we excede the limit of the learning rate from 0.001 to 1 or above, in that case we might overshot the global minima.\n",
    "\n",
    "- Learning Rate = 0.001\n",
    "\n",
    "- M >> Slope >> coifficeinet\n",
    "- C >> Intercept >> Y-Intercept\n",
    "\n",
    "- M  >> PD wrt to M\n",
    "- C  >> PD wrt to C"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08947a43",
   "metadata": {},
   "source": [
    "- 1. Batch Gradient Descent\n",
    "        Batch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples:\n",
    "        \n",
    "- 2. Mini-batch Gradient Descent\n",
    "        Instead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on the batch size. Therefore, learning happens on each mini-batch of b             examples: \n",
    "        Shuffle the training data set to avoid pre-existing order of examples.\n",
    "        Partition the training data set into b mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch.\n",
    "        \n",
    " \n",
    "- 3. Stochastic Gradient Descent\n",
    "        Instead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example (x^i,y^i). Therefore, learning happens on every example:\n",
    "\n",
    "        Shuffle the training data set to avoid pre-existing order of examples.\n",
    "        Partition the training data set into m examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70296070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f29e85d3",
   "metadata": {},
   "source": [
    "### - Evaluation of Linear Regression Model\n",
    "- 1. r2_score\n",
    "- 2. adjusted r2_score\n",
    "- 3. Root Mean Squared Error (RMSE)\n",
    "- 4. Mean Squared Error (MSE)\n",
    "- 5. Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748a8df",
   "metadata": {},
   "source": [
    "- SSE (Sum Of Squared Error)\n",
    "- SSR (Sum Of Squared Error Due To Regression)\n",
    "- SST (Sum Of Total Error (SST))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73551a46",
   "metadata": {},
   "source": [
    "- 1. Sum Of Squared Error (SSE) :- \n",
    "    - Sum of Squared Error it is the distance between Observed data point to Predicted data points.\n",
    "    - Sum(Yact - Ypred)^2\n",
    "    \n",
    "- 2. Sum Of Squared Error Due To Regression (SSR) :- \n",
    "    - it is the Distance between Sum of Ypredicted data points to Ymean \n",
    "    - Sum(Ypred - Ymean)^2\n",
    "    \n",
    "- 3. Sum Of Total Error (SST) :-\n",
    "    - It is the Total Distance between Observed data point to Mean Y\n",
    "    - Sum(Yact - Ymean)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d7573e",
   "metadata": {},
   "source": [
    "##### 1. r2_score"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46285a96",
   "metadata": {},
   "source": [
    "- It is a Linear regression model Evaluation metrics. this metrics comes with the sklearn library, \n",
    "- It is also known as Coifficient of Ditermination\n",
    "- it is Scale Invariant i.e it's scoring varris between 0 to 1 \n",
    "- This measures the amount of variation that can be explained by our model i.e. percentage of correct predictions returned by our model. It is also called the coefficient of determination and calculated by the formula\n",
    "- r2_score can be negative if the value of SSE is greatter than the SSE\n",
    "\n",
    "- Equations :- \n",
    "    r2_score = 1 - SSE / SST   ≈   (SST - SSE) / SST\n",
    "    also, r2_score = 1 -  Unexplained variation / Total Variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b33a2f",
   "metadata": {},
   "source": [
    "##### 2. Adjusted r2_score"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8ae2a7c",
   "metadata": {},
   "source": [
    "- we have to claculate adjusted r2_score manually, it dosen't come with the sklearn library\n",
    "- we need to calculate the adjusted r2_score  because to highlight the variation in the score whenever we add the correlated or Non-Correlated features into the dataset.\n",
    "\n",
    "- Equestions:- \n",
    "    - Adjusted r2_score = r2_score - (k-1) * (1-r2_score) / (N - K)\n",
    "    - N = Numebr of sample size\n",
    "    - K = number of Input Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ab601",
   "metadata": {},
   "source": [
    "#### Difference between r2_score and adjusted r2_score "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7bb5433",
   "metadata": {},
   "source": [
    "- r2_score\n",
    "    - 1. r2_score is metrics of sklearn library\n",
    "    - 2. if we add new corelated-feature into the dataset then the accuracy will get increase.\n",
    "    - 3. if we add NON - corelated-feature into the dataset then the accurasy will reamine the same as it is. or constant (No change)\n",
    "    \n",
    "- Adjusted r2_score\n",
    "    - 1. Adjusted r2_score is not in-build library in  sklearn package. we have to impliment it manually.(manual process)\n",
    "    - 2. if we add new corelated-feature into the dataset then the accuracy will get INCREASE.\n",
    "    - 3. if we add NON - corelated-feature into the dataset then the accurasy will get change (change)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1b628a",
   "metadata": {},
   "source": [
    "##### Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f141c54",
   "metadata": {},
   "source": [
    "- Mean absolute error, also known as L1 loss is one of the simplest loss functions and an easy-to-understand evaluation metric. It is calculated by taking the absolute difference between the predicted values and the actual values and averaging it across the dataset. Mathematically speaking, it is the arithmetic average of absolute errors. MAE measures only the magnitude of the errors and doesn’t concern itself with their direction. The lower the MAE, the higher the accuracy of a model.\n",
    "\n",
    "Equestion :- \n",
    "    - MAE = Sum(|Yact - Ypred|) / n\n",
    "    - n = Number of Observations or rows\n",
    "    - Yact = Yactual values or observation values of Target variable\n",
    "    - Ypred = Predicted values (values predicted by our regression model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ab49b",
   "metadata": {},
   "source": [
    "##### Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08f56965",
   "metadata": {},
   "source": [
    "MSE is one of the most common regression loss functions. In Mean Squared Error also known as L2 loss, we calculate the error by squaring the difference between the predicted value and actual value and averaging it across the dataset. MSE is also known as Quadratic loss as the penalty is not proportional to the error but to the square of the error. Squaring the error gives higher weight to the outliers, which results in a smooth gradient for small errors. Optimization algorithms benefit from this penalization for large errors as it is helpful in finding the optimum values for parameters. MSE will never be negative since the errors are squared. The value of the error ranges from zero to infinity. MSE increases exponentially with an increase in error. A good model will have an MSE value closer to zero.\n",
    "\n",
    "Equestion :- \n",
    "    - MSE = Sum(Yact - Ypred)^2 / n\n",
    "    - n = Number of Observations or rows\n",
    "    - Yact = Yactual values or observation values of Target variable\n",
    "    - Ypred = Predicted values (values predicted by our regression model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441ddc4",
   "metadata": {},
   "source": [
    "##### Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee2ac9c5",
   "metadata": {},
   "source": [
    "RMSE is computed by taking the square root of MSE. RMSE is also called the Root Mean Square Deviation. It measures the average magnitude of the errors and is concerned with the deviations from the actual value. RMSE value with zero indicates that the model has a perfect fit. The lower the RMSE, the better the model and its predictions. A higher RMSE indicates that there is a large deviation from the residual to the ground truth. RMSE can be used with different features as it helps in figuring out if the feature is improving the model’s prediction or not.\n",
    "\n",
    "It penalizes the model more if the predicted value is less than the actual value while the model is less penalized if the predicted value is more than the actual value. It does not penalize high errors due to the log.\n",
    "\n",
    "Equestion :- \n",
    "    - MSE = √Sum(Yact - Ypred)^2 / n\n",
    "    - n = Number of Observations or rows\n",
    "    - Yact = Yactual values or observation values of Target variable\n",
    "    - Ypred = Predicted values (values predicted by our regression model)\n",
    "    \n",
    "    - np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934e710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef791937",
   "metadata": {},
   "source": [
    "#### Difference between Covariance, Variance, Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960db581",
   "metadata": {},
   "source": [
    "#### 1.  Covariance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fb200af",
   "metadata": {},
   "source": [
    "- Covariance shows how the two variables differ. \n",
    "- Covariance indicates the relationship of two variables whenever one variable changes. If an increase in one variable results in an increase in the other variable, both variables are said to have a positive covariance. Decreases in one variable also cause a decrease in the other.\n",
    "    - (X ↑, & Y ↑ = +ve covariance)\n",
    "    \n",
    "    - (X ↑, & Y ↓ = -ve covariance)\n",
    "    \n",
    ">>> NOTE :- It tells you that what is the direction of the variables is, But is dosent show how much it is +vely correlated  and  how much it is -ve correlated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385fda4",
   "metadata": {},
   "source": [
    "#### 2. Variance "
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb44df88",
   "metadata": {},
   "source": [
    "- It is the Error's in a training data points\n",
    "- variance is a measure of how far observed values differ from the average of predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f170f",
   "metadata": {},
   "source": [
    "#### 3. Correlation (Pearson Coifficient of Corelations)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7019abd",
   "metadata": {},
   "source": [
    "- Correlation (Pearson cofficient of correlations or cofficient of corelation)\n",
    "- correlation shows you how the two variables are related\n",
    ">>> NOTE :- it shows how strongly variables is corelated (X and Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d5df06",
   "metadata": {},
   "source": [
    "#### What is Hyper Tuninig"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1585f40",
   "metadata": {},
   "source": [
    "- in L1 and L2 Regularization\n",
    "- Hyperparameter tuning consists of finding a set of optimal hyperparameter values for a learning algorithm while applying this optimized algorithm to any data set. That combination of hyperparameters maximizes the model's performance, minimizing a predefined loss function to produce better results with fewer errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a745d29",
   "metadata": {},
   "source": [
    "###  Two Methods to deal with UnderFitting and OverFitting\n",
    "- Explaination:- \n",
    "https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/\n",
    "\n",
    "- There are two methods to handal UnderFitting and Overfitting conditions in our Linear regression\n",
    "    - 1. Lasso Regression (L1 Regularisation)\n",
    "    - 2. Ridge Regression (L2 Regularisation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6091d17",
   "metadata": {},
   "source": [
    "### 1. Lasso regression\n",
    "1. Lasso regression\n",
    "    - When we have 60 - 100 Features in our dataset in that case we can use L1- Regularisation also known as Lasso regression.\n",
    "        - It is also use as a feature selection technique\n",
    "- Formula \n",
    "    - [ (Ya - Yp)^2 +  λ * | slope | ]\n",
    "    -  λ = 0.01 to ∞\n",
    "    \n",
    "###### NOTE :- If the  λ = 0, Then it will work like Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9a053",
   "metadata": {},
   "source": [
    "- lasso regression is use to not only for overfitting condition but also it help us to do feature selection.\n",
    "- we are using magnitude of the slope (λ+|slope|), and where the slope value is very less those feature will be removed \n",
    "\n",
    "###### NOTE:- In Case of Lasso regression the magnitude of the sum of slope feature which are not important ( |M1X1 + M2X2+M3X3+...MnXn| ) will become zero after some time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795435d",
   "metadata": {},
   "source": [
    "### 3. Ridge Regression (L2 - Regression)\n",
    "\n",
    "- Ridge Regression\n",
    "     - when we having Overfitting in our model then we can use Ridge Regression also known as L2-Regression\n",
    "- Formula\n",
    "    - θ ~ Slope\n",
    "    - (Ya - Yp)^2 + λ * ( slope )^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d2c2b",
   "metadata": {},
   "source": [
    "- In Ridge Regression we penalize (we are trying to make the slope less steeper by  λ (lambda) , in order to minimize the cost function by adding ***λ + (slope)^2***, i.e penalizing.) the the feature which are having a higher slope values. this is how overfitting got reduced by ridge regression.\n",
    "- as the the λ value goes higher, the regression line goes very very close to the 0.\n",
    "- λ value is selected by using cross validation (cv)\n",
    "\n",
    "\n",
    "###### NOTE:- in ridge regression will move towords the zero but it will never reached the zero (it will shrink the value but not exactly zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149039a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
