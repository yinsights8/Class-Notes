{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400fabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langdetect\n",
    "! pip install collections-extended\n",
    "! pip install googletrans\n",
    "! pip install seaborn \n",
    "! pip install contractions\n",
    "! pip install yake\n",
    "! pip install rake_nltk\n",
    "! pip install unidecode\n",
    "! pip install googletrans==3.1.0a0\n",
    "! pip install autocorrect\n",
    "! pip install gensim\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd3e06",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4bcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from langdetect import detect  # ot detect the language \n",
    "from googletrans import Translator # to tranlate the language\n",
    "from collections import Counter # get the frequency of words\n",
    "from nltk.corpus import stopwords # remove the stopwords from the text\n",
    "from nltk.util import ngrams # genate ngrams\n",
    "from wordcloud import WordCloud  # generate the WordCloud\n",
    "from string import punctuation # remove the punctuations from the text\n",
    "import contractions #  to separate the words like didn't, haven't etc..\n",
    "import yake         # get the score of the important features (summary generation)\n",
    "from rake_nltk import Rake # # get the score of the important features (summary generation)\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer # it is used to reduce words to their base form, also known as the root form\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, WhitespaceTokenizer, regexp_tokenize # convert the text into words of list\n",
    "from unidecode import unidecode # it is use to handle the accented charaters\n",
    "from gensim.models import Word2Vec # is to group the vectors of similar words together in vectorspace\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.metrics import silhouette_score # score of Kmeans cluster\n",
    "from yellowbrick.cluster import silhouette_visualizer # score of Kmeans cluster\n",
    "from sklearn.feature_extraction.text import  CountVectorizer, TfidfVectorizer # it will return the text into sparse matrix"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9afd30d7",
   "metadata": {},
   "source": [
    "1. Data formats\n",
    "\t- 1. json\n",
    "    - 2. csv\n",
    "\t- 3. txt\n",
    "    - 4. images\n",
    "    \n",
    "Problem statement"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c02c8d2",
   "metadata": {},
   "source": [
    "2. Initial stage\n",
    "- 1. language detect\n",
    "- 2. language translation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48bc6b4d",
   "metadata": {},
   "source": [
    "3.\n",
    "A. Ngrams \n",
    "\t- unigrams\n",
    "    - bigrams\n",
    "    - trigrams, etc.\n",
    "    - 1. Removing domain specific keywords\n",
    "    - 2. keyphrases\n",
    "- B. wordcloud\n",
    "- C. Keyphrase Extraction\n",
    "    - 1. YAKE\n",
    "    - 2. RAKE"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4aee89cc",
   "metadata": {},
   "source": [
    "4. Preprocessing\n",
    "\t1. removing spaces, newline and blankline\n",
    "    2. Contraction Mapping\n",
    "    3. Handling Accented Characters\n",
    "    4. Cleaning\n",
    "    \ta. Tokenization\n",
    "        \t- sentence tokenization  *\n",
    "            - word tokenization      *\n",
    "            - whitespace tokenization\n",
    "            - RegEx tokenization\n",
    "        b. remove Punctuations\n",
    "        c. Remove Stopwords\n",
    "        \t- language specific\n",
    "            - Domain specific\n",
    "        d. len(word) > 2\n",
    "        e. Normalize\n",
    "    5. Autocorrection\n",
    "    \t- autocorrected\n",
    "        - textblob\n",
    "        - symspell\n",
    "    6. Stemming or Lemmetization (to get the root words from the text)\n",
    "    \t- porter stemmer\n",
    "        - snowball stemmer\n",
    "\n",
    "\n",
    ">>> After all of the above steps we will get Clean data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53b553bd",
   "metadata": {},
   "source": [
    "after geting the clean data we will check for \n",
    "\n",
    "1. If clean data has target column (Labled data)\n",
    "\t- Model build\n",
    "    \n",
    "1. if we don't have target column (Unlabled data) \n",
    "\t- a. Kmeans clusturing\n",
    "    \t- CountVectorizer\n",
    "        - TfIdf vectorizer\n",
    "        - word2vec\n",
    "      b. hierarchical clusturing\n",
    "      c. DB scan\n",
    "      d. SOM (Self Organising map)\n",
    "\n",
    "2. Support columns \n",
    "\tto create a target column we need to find the support columns \n",
    "    like 'Star rating' etc. \n",
    "    \n",
    "3. Evaluation :-\n",
    "\t1. Shilhouette score\n",
    "    2. silhouitte visualizer\n",
    "    3. dunn index\n",
    "    4. kappa index"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dea1d249",
   "metadata": {},
   "source": [
    "- Training data\n",
    "\n",
    "- Modeling \n",
    "\t- logistic regression\n",
    "    - SVM \n",
    "    - decision tree\n",
    "    - adaboost\n",
    "    - naive bayes\n",
    "    \n",
    "- evaluation\n",
    "\n",
    "- create pickle file\n",
    "- deployement\n",
    "- CICD"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b2b9647",
   "metadata": {},
   "source": [
    "CICD -> Continuous integration contineours deployement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c52e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
