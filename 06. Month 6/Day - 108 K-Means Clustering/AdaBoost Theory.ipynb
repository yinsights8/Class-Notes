{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f5498d",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa2be416",
   "metadata": {},
   "source": [
    "1. Bagging >> Parallel Approach\n",
    "2. Boosting >> Sequential Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640be234",
   "metadata": {},
   "source": [
    "# Boosting Algorithms"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0abbcd12",
   "metadata": {},
   "source": [
    "1. Gradient Boosting\n",
    "2. AdaBoosting'\n",
    "3. XGBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b2a2a8",
   "metadata": {},
   "source": [
    "# Adaboost Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47f5b0aa",
   "metadata": {},
   "source": [
    "Adaboosting do prefer Sequential approach and it comes under ensemble method.\n",
    "\n",
    "1. It is Supervised Machine Learning Algorithm\n",
    "2. It is useful for classification as well as Regression\n",
    "3. In Adaboost we need to assign the weights. While passing data to the first week learner.\n",
    "4. Initial weights on the data will get assigned on the basis of standard formula. i.e. (1/n)\n",
    "   n = Total number of samples which is present in dataset\n",
    "5. When first week learner will get trained. On the data which we have passed  After that. The same\n",
    "   Data which we can pass to the next week leaner after few modifications in the weights.\n",
    "6. We always focus on the mistakes or misclassifications in the adaboost. That means, Needs to increase\n",
    "   the weights of misclassified data points and needs to decrease the weights of correctly classified\n",
    "   datapoints.\n",
    "7. Each and every single step while passing the data to different weak leaners. Always focus on the\n",
    "   Misclassification for getting the good accuracy and and to clarify the mistakes\n",
    "8. So for updating the weights in each step. We are having standard formulas.\n",
    "9. Once the weights will get assigned. Model starts it's training on the available data by using the\n",
    "   stumps.\n",
    "    \n",
    "\n",
    "    \n",
    "Forest of Stumps:\n",
    "    \n",
    "    \n",
    "1. Stumps >> A tree with just one branch node and two leaf nodes are called as Stumps\n",
    "2. Stumps are weak learners or Base Learners\n",
    "3. Here, In Sequential approach. Model 2 is depends on the model 1\n",
    "4. Each stump will get trained by taking the previous stumps mistake in it's account. Due to which\n",
    "   we can increase the accuracy step wise.\n",
    "5. Here. We are converting weak nodes in to Strong nodes for better accuracy."
   ]
  },
  {
   "cell_type": "raw",
   "id": "30d177e6",
   "metadata": {},
   "source": [
    "Sample Weight = 1/n      n = Total Number of samples\n",
    "\n",
    "\n",
    "Model Performance = 1/2 * loge((1 - TE)/TE)\n",
    "\n",
    "Total Error (TE) = Number of misclassified data points / Total Number of data points\n",
    "\n",
    "New_Sample_Weights (Misclassified Data points) = old weight * E^(Model Performance)\n",
    "\n",
    "New_Sample_Weights (Correctly Classified Datapoints) = old weight * E^(-Model Performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
