{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed67697",
   "metadata": {},
   "source": [
    "# ROC and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a2262f",
   "metadata": {},
   "source": [
    "ROC  >> Receiver Operating Curve\n",
    "AUC  >> Area under the Curve\n",
    "\n",
    "We always expect to get the maximum area under the ROC. SO that we can consider,Our Model is the \n",
    "best model.\n",
    "The ROC and AUC that we can plot by using the thresh on both the parameters. Like FPR and TPR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f760f",
   "metadata": {},
   "source": [
    "# Underfitting and Overfitting"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18e4b7cb",
   "metadata": {},
   "source": [
    "Underfitting : When model neither learn from training dataset and nor from testing dataset. That\n",
    "    concern is nothing but the Underfitting.\n",
    "    \n",
    "    \n",
    "    Training Accuracy >> Low\n",
    "    Testing Accuracy  >> Low\n",
    "    \n",
    "    Training Accuracy >> 65%\n",
    "    Testing Accuracy  >> 60%\n",
    "    \n",
    "    \n",
    "    \n",
    "Overfitting : When model learn well on training dataset and not perform well on an unseen dataset\n",
    "    or testing dataset. Then this is called as Overfitting.\n",
    "    \n",
    "    \n",
    "    Training Accuracy >> High\n",
    "    Testing Accuracy  >> Low\n",
    "    \n",
    "    Training Accuracy >> 95%\n",
    "    Testing Accuracy  >> 83% or 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ca177",
   "metadata": {},
   "source": [
    "# Bias and Variance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e6fe179",
   "metadata": {},
   "source": [
    "Bias : Bias is nothing but the Training errors. We always try to keep the low Bias. That means low\n",
    "    training errors.\n",
    "    \n",
    "    \n",
    "    High Training Accuracy >> LOw Bias\n",
    "    Low Training Accuracy  >> High Bias\n",
    "    \n",
    "    \n",
    "\n",
    "Variance : The Difference between the accuracies of different datasets (Training and Testing Datasets)\n",
    "    \n",
    "    \n",
    "    Low Variance :\n",
    "        \n",
    "        1. Training Accuracy >> High >> 98%\n",
    "        2. Testing Accuracy  >> High >> 97%   -------> Best Model\n",
    "        \n",
    "        \n",
    "    High Variance :\n",
    "        \n",
    "        1. Training Accuracy >> High  >> 95%\n",
    "        2. Testing Accuracy  >> Low   >> 80%\n",
    "        \n",
    "        \n",
    "    Low Variance :\n",
    "        \n",
    "        1. Training Accuracy >> Low >> 65%\n",
    "        2. Testing Accuracy  >> Low >> 63%  --------> Underfitting\n",
    "        \n",
    "        \n",
    "Underfitting  >> High Bias and Low Variance\n",
    "Overfitting   >> Low Bias  and High Variance\n",
    "\n",
    "\n",
    "Best Model  >> Low Bias and Low Variance\n",
    "        \n",
    "    \n",
    "Bias and Variance Error >> B^2 + V + IR\n",
    "\n",
    "B >> Bias\n",
    "V >> Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0001ccc9",
   "metadata": {},
   "source": [
    "# How to Avoid Underfitting and Overfitting"
   ]
  },
  {
   "cell_type": "raw",
   "id": "680ec2c1",
   "metadata": {},
   "source": [
    "1. Underfitting :\n",
    "    \n",
    "    Low Training Accuracy\n",
    "    Low Testing Accuracy\n",
    "    \n",
    "1. Add more Parameters (20 >> 25 or 30)\n",
    "2. Use Correlated Features\n",
    "3. Remove Outliers\n",
    "4. Replace the missing values by mean, Median and Mode\n",
    "5. Increase the Data Size (20000 >> 25000)\n",
    "\n",
    "\n",
    "\n",
    "2. Overfitting :\n",
    "    \n",
    "    High training Accuracy\n",
    "    Low Testing Accuracy\n",
    "    \n",
    "    \n",
    "1. Use less Parameters\n",
    "2. Use Hyperparameter Tuning\n",
    "3. We can Regularization Techniques (L1 and L2 Regularization)\n",
    "4. Remove Outliers\n",
    "5. We can go ahead with the Cross Validation Technique\n",
    "6. Increase the data size (10000 >> 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84619f",
   "metadata": {},
   "source": [
    "# Assumptions of Logistic Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc3a526a",
   "metadata": {},
   "source": [
    "1. Independence\n",
    "2. No Multicolinearity\n",
    "3. Linear Relationship between independent variables and Logit odd\n",
    "\n",
    "log(p/1-p)   >> -infi to +infi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82664aa3",
   "metadata": {},
   "source": [
    "# Advantages of Logistic Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9ff0c78",
   "metadata": {},
   "source": [
    "1. It is easy to understand (We use sigmoid function and LogLoss function in Logistic Regression)\n",
    "2. Less Likely to be overfitted\n",
    "3. When model will overfit then in that case, we can go ahead with Regularization Techniques (L1 and L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3088a",
   "metadata": {},
   "source": [
    "# Disadvantages of Logistic Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bb1e2fb",
   "metadata": {},
   "source": [
    "1. Lot of Feature engineering od Required.\n",
    "2. When independent variables are highly correlated to each other, It may affect on the performance\n",
    "of the model.\n",
    "3. Highly sensitive to Outliers\n",
    "4. We can not perform or use Logistic Regression on less amount of Data (Number of Rows == Numbers of Columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
